{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982d70f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "982d70f3",
    "outputId": "ee4dafe5-381b-4a10-d048-e6f17e194812"
   },
   "outputs": [],
   "source": [
    "# ! pip install pymupdf\n",
    "# ! pip install PyPDF2\n",
    "# ! pip install sentence-transformers\n",
    "# ! pip install faiss-cpu\n",
    "# ! pip install transformers\n",
    "# ! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "_UhHzmSzYpZp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UhHzmSzYpZp",
    "outputId": "29439a5e-72d7-4a5c-ae05-63c49a9480fd"
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import fitz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397438f",
   "metadata": {
    "id": "4397438f",
    "lines_to_next_cell": 2
   },
   "source": [
    "# 1. Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9a3b4e",
   "metadata": {
    "id": "ec9a3b4e"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by removing extra whitespace and newline characters.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed text with newline characters replaced by spaces and\n",
    "         multiple spaces reduced to a single space.\n",
    "    \"\"\"\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce8dfef",
   "metadata": {
    "id": "5ce8dfef"
   },
   "outputs": [],
   "source": [
    "def pdf_to_text(path, start_page=1, end_page=None):\n",
    "    \"\"\"\n",
    "    Extract text from specified pages of a PDF file.\n",
    "\n",
    "    This function opens a PDF file located at the specified `path`, extracts the text\n",
    "    from pages ranging from `start_page` to `end_page` (inclusive), preprocesses the text,\n",
    "    and returns a list of text strings, one for each page.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The path to the PDF file.\n",
    "    start_page (int, optional): The starting page number (1-based index) from which\n",
    "                                 to start extracting text. Defaults to 1.\n",
    "    end_page (int, optional): The ending page number (1-based index) up to which\n",
    "                               text should be extracted. If None, text is extracted\n",
    "                               until the last page. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    list of str: A list where each element is a string containing the preprocessed\n",
    "                 text from a corresponding page of the PDF.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(path)\n",
    "    total_pages = doc.page_count\n",
    "\n",
    "    if end_page is None:\n",
    "        end_page = total_pages\n",
    "\n",
    "    text_list = []\n",
    "\n",
    "    for i in range(start_page - 1, end_page):\n",
    "        text = doc.load_page(i).get_text(\"text\")\n",
    "        text = preprocess(text)\n",
    "        text_list.append(text)\n",
    "\n",
    "    doc.close()\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48933582",
   "metadata": {
    "id": "48933582"
   },
   "outputs": [],
   "source": [
    "def text_to_chunks(texts, word_length=50, overlap=10, start_page=1):\n",
    "    \"\"\"\n",
    "    Split text from multiple pages into chunks of specified word length with overlap.\n",
    "\n",
    "    This function takes a list of text strings (one for each page), splits each text\n",
    "    into chunks of a specified word length with a specified overlap, and formats these\n",
    "    chunks with page numbers for easy reference. If a chunk on a page is smaller than\n",
    "    the specified word length and there are more pages, it concatenates this chunk\n",
    "    with the beginning of the following page's text.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list of str): A list where each element is a string representing the text\n",
    "                         of a page.\n",
    "    word_length (int, optional): The maximum number of words per chunk. Defaults to 50.\n",
    "    overlap (int, optional): The number of words to overlap between chunks. Defaults to 10.\n",
    "    start_page (int, optional): The starting page number to use for formatting the page\n",
    "                                number in the chunk. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "    list of str: A list of text chunks where each chunk is a string formatted with the\n",
    "                 page number and the chunked text content.\n",
    "    \"\"\"\n",
    "    text_toks = [t.split(' ') for t in texts]\n",
    "    chunks = []\n",
    "\n",
    "    for idx, words in enumerate(text_toks):\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            # Create a chunk of the specified length\n",
    "            chunk = words[i:i + word_length]\n",
    "\n",
    "            # If this is the last chunk and it's shorter than word_length, check if we need to merge with the next page\n",
    "            if (i + word_length) > len(words) and len(chunk) < word_length and (len(text_toks) != (idx + 1)):\n",
    "                text_toks[idx + 1] = chunk + text_toks[idx + 1]\n",
    "                break\n",
    "\n",
    "            # Join the chunk into a string and format it with the page number\n",
    "            chunk = ' '.join(chunk).strip()\n",
    "            chunk = f'[Page no. {idx + start_page}]' + ' ' + '\"' + chunk + '\"'\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            # Move the index forward by the word length minus the overlap\n",
    "            i += word_length - overlap\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7yUOi5GGawys",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yUOi5GGawys",
    "outputId": "2e9b064d-2652-4d39-c774-0bb486ef31ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u393845\\venv_xai\\lib\\site-packages\\urllib3\\connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'fleuret.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded and saved to dl_book.pdf\n"
     ]
    }
   ],
   "source": [
    "# URL of the PDF file you want to download\n",
    "pdf_url = \"https://fleuret.org/public/lbdl.pdf\"\n",
    "# Download the PDF file\n",
    "response = requests.get(pdf_url, verify = False)\n",
    "\n",
    "# Save the PDF to the Colab filesystem\n",
    "path = 'dl_book.pdf'\n",
    "\n",
    "with open(path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(f\"PDF downloaded and saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbbc58dd",
   "metadata": {
    "id": "bbbc58dd"
   },
   "outputs": [],
   "source": [
    "#indexing two chapters of this book\n",
    "start_page = 1\n",
    "end_page = None\n",
    "texts = pdf_to_text(path, start_page=start_page, end_page=end_page)\n",
    "chunks = text_to_chunks(texts, start_page=start_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d38cf0e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d38cf0e6",
    "outputId": "2d33623c-7c3b-45f2-f48e-b7cdbbf7196a"
   },
   "outputs": [],
   "source": [
    "#print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03f208",
   "metadata": {
    "id": "4f03f208",
    "lines_to_next_cell": 2
   },
   "source": [
    "# 2. Embeddings and Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86abb18f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86abb18f",
    "outputId": "48b7b606-7ba0-4649-9d98-873628878297"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u393845\\venv_xai\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "# Create FAISS index\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add database vectors to the index\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Store chunk mappings\n",
    "chunk_mapping = {i: chunk for i, chunk in enumerate(chunks)}\n",
    "#print(chunk_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17e40b",
   "metadata": {
    "id": "1b17e40b"
   },
   "source": [
    "# 3. RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52ad7a",
   "metadata": {
    "id": "4c52ad7a",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = \"t5-large\"\n",
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b728e41",
   "metadata": {
    "id": "4b728e41",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    \"\"\"\n",
    "    Generate an answer to a given question based on relevant chunks of text.\n",
    "\n",
    "    This function retrieves relevant chunks of text related to the given question,\n",
    "    concatenates them with the question to form the input for a generative model,\n",
    "    and produces an answer using that model.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Encodes the question into an embedding.\n",
    "    2. Searches for the most similar text chunks based on the embedding.\n",
    "    3. Concatenates the retrieved chunks and the question to form the input for\n",
    "       the generative model.\n",
    "    4. Generates an answer using the model and returns it.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The question for which an answer is to be generated.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated answer in response to the question.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant chunks\n",
    "    question_embedding = model.encode([question])\n",
    "    D, I = index.search(np.array([question_embedding]).reshape(np.array([question_embedding]).shape[0],np.array([question_embedding]).shape[2]), k=5)\n",
    "    retrieved_chunks = [chunk_mapping[i] for i in I[0]]\n",
    "\n",
    "    # Generate answer\n",
    "    input_text = \" \".join(retrieved_chunks) + \" \" + question\n",
    "\n",
    "    #input_text = \"question:\" + question + \"context:\" + \" \".join(retrieved_chunks)\n",
    "    input_text = \"Question: \" + question + \"\\nContext: \" + \" \".join(retrieved_chunks) + \"\\nAnswer:\"\n",
    "\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model1.generate(inputs, max_length=150, num_return_sequences=1)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a31e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc5a31e4",
    "outputId": "1f1dd9a9-66e0-4860-b64d-321357ae9477"
   },
   "outputs": [],
   "source": [
    "# Test the pipeline\n",
    "question = \"What is this book about?\"\n",
    "answer = generate_answer(question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f70f5f",
   "metadata": {},
   "source": [
    "# 4. Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ce975",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "e67ce975",
    "lines_to_next_cell": 2,
    "outputId": "c7a736aa-bcbe-4abb-d73b-c346dca4c5bb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of pre-defined questions\n",
    "questions = [\n",
    "    \"Who is the author of this book?\",\n",
    "    \"What is the official URL of this book?\",\n",
    "    \"What was the date when this book was published?\",\n",
    "    \"Who proposed the Variational Autoencoder?\"\n",
    "]\n",
    "\n",
    "# Function to handle custom and pre-defined questions\n",
    "def handle_question(input_question, selected_question):\n",
    "    \"\"\"\n",
    "    Handle and generate an answer for a question based on user input.\n",
    "\n",
    "    This function determines the appropriate question to use based on whether a\n",
    "    `selected_question` is provided. If a `selected_question` is provided, it is\n",
    "    used as the question. Otherwise, the `input_question` is used. If no question\n",
    "    is provided, the function returns a prompt asking the user to enter or select\n",
    "    a question. Otherwise, it generates and returns an answer using the\n",
    "    `generate_answer` function.\n",
    "\n",
    "    Parameters:\n",
    "    input_question (str): The question provided by the user as input.\n",
    "    selected_question (str, optional): The question selected by the user from a list\n",
    "                                        or options. If provided, it takes precedence\n",
    "                                        over `input_question`.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated answer in response to the selected or input question, or\n",
    "         a prompt if no question is provided.\n",
    "    \"\"\"\n",
    "\n",
    "    if selected_question:\n",
    "        question = selected_question\n",
    "    else:\n",
    "        question = input_question\n",
    "\n",
    "    if not question:\n",
    "        return \"Please enter or select a question.\"\n",
    "\n",
    "    return generate_answer(question)\n",
    "\n",
    "# Create Gradio interface\n",
    "def create_interface():\n",
    "    \"\"\"\n",
    "    Create a Gradio interface for question answering.\n",
    "\n",
    "    This function sets up a Gradio interface that allows users to enter a question\n",
    "    manually or select a pre-defined question from a dropdown menu. Users can submit\n",
    "    their question by clicking a button, and the interface will display the generated\n",
    "    answer in a non-interactive textbox.\n",
    "\n",
    "    The interface includes:\n",
    "    - A textbox for entering a question.\n",
    "    - A dropdown menu for selecting from pre-defined questions.\n",
    "    - A submit button to trigger the question handling and answer generation.\n",
    "    - An output textbox to display the generated answer.\n",
    "\n",
    "    Returns:\n",
    "    gr.Blocks: A Gradio `Blocks` object representing the created interface.\n",
    "    \"\"\"\n",
    "    with gr.Blocks() as demo:\n",
    "        with gr.Row():\n",
    "            question_textbox = gr.Textbox(label=\"Enter Your Question\")\n",
    "            question_dropdown = gr.Dropdown(choices=questions, label=\"Or Select a Pre-defined Question\")\n",
    "        with gr.Row():\n",
    "            answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
    "        submit_button = gr.Button(\"Submit\")\n",
    "\n",
    "        submit_button.click(\n",
    "            fn=handle_question,\n",
    "            inputs=[question_textbox, question_dropdown],\n",
    "            outputs=answer_output\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "# Launch the Gradio app\n",
    "interface = create_interface()\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089756b",
   "metadata": {
    "id": "8089756b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv_xai",
   "language": "python",
   "name": "venv_xai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# -*- coding: utf-8 -*-
"""RAG_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WfZcRtnuYPERMYlyH7-vOQZ14Pt9wV-T
"""

# ! pip install pymupdf
# ! pip install PyPDF2
# ! pip install sentence-transformers
# ! pip install faiss-cpu
# ! pip install transformers
# ! pip install gradio

from PyPDF2 import PdfReader
import fitz
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import re
import requests
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import gradio as gr

"""# 1. Text Extraction"""

def preprocess(text):
    """
    Preprocess the input text by removing extra whitespace and newline characters.

    Parameters:
    text (str): The input text to be preprocessed.

    Returns:
    str: The preprocessed text with newline characters replaced by spaces and
         multiple spaces reduced to a single space.
    """
    text = text.replace('\n', ' ')
    text = re.sub('\s+', ' ', text)
    return text

def pdf_to_text(path, start_page=1, end_page=None):
    """
    Extract text from specified pages of a PDF file.

    This function opens a PDF file located at the specified `path`, extracts the text
    from pages ranging from `start_page` to `end_page` (inclusive), preprocesses the text,
    and returns a list of text strings, one for each page.

    Parameters:
    path (str): The path to the PDF file.
    start_page (int, optional): The starting page number (1-based index) from which
                                 to start extracting text. Defaults to 1.
    end_page (int, optional): The ending page number (1-based index) up to which
                               text should be extracted. If None, text is extracted
                               until the last page. Defaults to None.

    Returns:
    list of str: A list where each element is a string containing the preprocessed
                 text from a corresponding page of the PDF.
    """
    doc = fitz.open(path)
    total_pages = doc.page_count

    if end_page is None:
        end_page = total_pages

    text_list = []

    for i in range(start_page - 1, end_page):
        text = doc.load_page(i).get_text("text")
        text = preprocess(text)
        text_list.append(text)

    doc.close()
    return text_list

def text_to_chunks(texts, word_length=150, overlap=10, start_page=1):
    """
    Split text from multiple pages into chunks of specified word length with overlap.

    This function takes a list of text strings (one for each page), splits each text
    into chunks of a specified word length with a specified overlap, and formats these
    chunks with page numbers for easy reference. If a chunk on a page is smaller than
    the specified word length and there are more pages, it concatenates this chunk
    with the beginning of the following page's text.

    Parameters:
    texts (list of str): A list where each element is a string representing the text
                         of a page.
    word_length (int, optional): The maximum number of words per chunk. Defaults to 50.
    overlap (int, optional): The number of words to overlap between chunks. Defaults to 10.
    start_page (int, optional): The starting page number to use for formatting the page
                                number in the chunk. Defaults to 1.

    Returns:
    list of str: A list of text chunks where each chunk is a string formatted with the
                 page number and the chunked text content.
    """
    text_toks = [t.split(' ') for t in texts]
    chunks = []

    for idx, words in enumerate(text_toks):
        i = 0
        while i < len(words):
            # Create a chunk of the specified length
            chunk = words[i:i + word_length]

            # If this is the last chunk and it's shorter than word_length, check if we need to merge with the next page
            if (i + word_length) > len(words) and len(chunk) < word_length and (len(text_toks) != (idx + 1)):
                text_toks[idx + 1] = chunk + text_toks[idx + 1]
                break

            # Join the chunk into a string and format it with the page number
            chunk = ' '.join(chunk).strip()
            chunk = f'[Page no. {idx + start_page}]' + ' ' + '"' + chunk + '"'
            chunks.append(chunk)

            # Move the index forward by the word length minus the overlap
            i += word_length - overlap

    return chunks

# URL of the PDF file you want to download
#pdf_url = 'https://assets.openstax.org/oscms-prodcms/media/documents/ConceptsofBiology-WEB.pdf?_gl=1*pba6kg*_gcl_au*MTE2NjU5NDcyMi4xNzIzMzk1NDk5*_ga*Nzk1NjI1NjI4LjE3MjMzOTU1MDE.*_ga_T746F8B0QC*MTcyMzM5NTUwMC4xLjEuMTcyMzM5NTUxMi40OC4wLjA.'
pdf_url = "https://fleuret.org/public/lbdl.pdf"
# Download the PDF file
response = requests.get(pdf_url, verify = False)

# Save the PDF to the Colab filesystem
path = 'dl_book.pdf'
with open(path, 'wb') as f:
    f.write(response.content)

print(f"PDF downloaded and saved to {path}")

#indexing two chapters of this book
start_page = 1
end_page = None
texts = pdf_to_text(path, start_page=start_page, end_page=end_page)
chunks = text_to_chunks(texts, start_page=start_page)

print(chunks)

"""# 2. Embeddings and Vector Storage"""

# Load model
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Compute embeddings
embeddings = model.encode(chunks)

# Create FAISS index
d = embeddings.shape[1]
index = faiss.IndexFlatL2(d)

# Add database vectors to the index
index.add(np.array(embeddings))

# Store chunk mappings
chunk_mapping = {i: chunk for i, chunk in enumerate(chunks)}
#print(chunk_mapping)

"""# 3. RAG Pipeline"""

# Load model
model_name = "t5-large"
model1 = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def generate_answer(question):
    """
    Generate an answer to a given question based on relevant chunks of text.

    This function retrieves relevant chunks of text related to the given question,
    concatenates them with the question to form the input for a generative model,
    and produces an answer using that model.

    The function performs the following steps:
    1. Encodes the question into an embedding.
    2. Searches for the most similar text chunks based on the embedding.
    3. Concatenates the retrieved chunks and the question to form the input for
       the generative model.
    4. Generates an answer using the model and returns it.

    Parameters:
    question (str): The question for which an answer is to be generated.

    Returns:
    str: The generated answer in response to the question.
    """
    # Retrieve relevant chunks
    question_embedding = model.encode([question])
    D, I = index.search(np.array([question_embedding]).reshape(np.array([question_embedding]).shape[0],np.array([question_embedding]).shape[2]), k=5)
    retrieved_chunks = [chunk_mapping[i] for i in I[0]]

    # Generate answer
    input_text = " ".join(retrieved_chunks) + " " + question

    #input_text = "question:" + question + "context:" + " ".join(retrieved_chunks)
    input_text = "Question: " + question + "\nContext: " + " ".join(retrieved_chunks) + "\nAnswer:"

    inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model1.generate(inputs, max_length=150, num_return_sequences=1)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# Test the pipeline
question = "What is this book about?"
answer = generate_answer(question)
print("Answer:", answer)

# List of pre-defined questions
questions = [
    "Who is the author of this book?",
    "What is the official URL of this book?",
    "What was the date when this book was published?",
    "Who proposed the Variational Autoencoder?"
]

# Function to handle custom and pre-defined questions
def handle_question(input_question, selected_question):
    """
    Handle and generate an answer for a question based on user input.

    This function determines the appropriate question to use based on whether a
    `selected_question` is provided. If a `selected_question` is provided, it is
    used as the question. Otherwise, the `input_question` is used. If no question
    is provided, the function returns a prompt asking the user to enter or select
    a question. Otherwise, it generates and returns an answer using the
    `generate_answer` function.

    Parameters:
    input_question (str): The question provided by the user as input.
    selected_question (str, optional): The question selected by the user from a list
                                        or options. If provided, it takes precedence
                                        over `input_question`.

    Returns:
    str: The generated answer in response to the selected or input question, or
         a prompt if no question is provided.
    """

    if selected_question:
        question = selected_question
    else:
        question = input_question

    if not question:
        return "Please enter or select a question."

    return generate_answer(question)

# Create Gradio interface
def create_interface():
    """
    Create a Gradio interface for question answering.

    This function sets up a Gradio interface that allows users to enter a question
    manually or select a pre-defined question from a dropdown menu. Users can submit
    their question by clicking a button, and the interface will display the generated
    answer in a non-interactive textbox.

    The interface includes:
    - A textbox for entering a question.
    - A dropdown menu for selecting from pre-defined questions.
    - A submit button to trigger the question handling and answer generation.
    - An output textbox to display the generated answer.

    Returns:
    gr.Blocks: A Gradio `Blocks` object representing the created interface.
    """
    with gr.Blocks() as demo:
        with gr.Row():
            question_textbox = gr.Textbox(label="Enter Your Question")
            question_dropdown = gr.Dropdown(choices=questions, label="Or Select a Pre-defined Question")
        with gr.Row():
            answer_output = gr.Textbox(label="Answer", interactive=False)
        submit_button = gr.Button("Submit")

        submit_button.click(
            fn=handle_question,
            inputs=[question_textbox, question_dropdown],
            outputs=answer_output
        )

    return demo

# Launch the Gradio app
interface = create_interface()
interface.launch()